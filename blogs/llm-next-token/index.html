<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="theme-color" content="#000000">
    <meta name="keywords"
        content="Prashant Pandey, LLM, large language models, tokenization, transformers, next token prediction">
    <meta name="language" content="English">
    <title>How Do LLMs Decide the Next Token? | Prashant Pandey</title>
    <meta name="description"
        content="Large Language Models (LLMs) like ChatGPT, Gemini, or Claude generate text one piece at a time. They don't write full sentences in one go.">

    <meta property="og:type" content="article">
    <meta property="og:url" content="https://prashantpandeyy.gitlab.io/blogs/llm-next-token/">
    <meta property="og:title" content="How Do LLMs Decide the Next Token?">
    <meta property="og:description"
        content="Large Language Models (LLMs) like ChatGPT, Gemini, or Claude generate text one piece at a time. They don't write full sentences in one go.">
    <meta property="og:image" content="https://prashantpandeyy.gitlab.io/assets/card.png">
    <meta property="og:site_name" content="Prashant Pandey Portfolio">

    <meta property="twitter:card" content="summary_large_image">
    <meta property="twitter:title" content="How Do LLMs Decide the Next Token?">
    <meta property="twitter:description"
        content="Large Language Models (LLMs) like ChatGPT, Gemini, or Claude generate text one piece at a time. They don't write full sentences in one go.">
    <meta property="twitter:image" content="https://prashantpandeyy.gitlab.io/assets/card.png">

    <meta name="author" content="Prashant Pandey">
    <link rel="canonical" href="https://prashantpandeyy.gitlab.io/blogs/llm-next-token/">

    <link rel="stylesheet" href="../../styles.css">
    <link rel="apple-touch-icon" sizes="180x180" href="/assets/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/assets/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/assets/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">

    <!-- Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-55N7G54DM5"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());
        gtag('config', 'G-55N7G54DM5');
    </script>

    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet" />
</head>

<body class="loading">
    <div class="theme-toggle" id="theme-toggle-btn">
        <svg id="moon-icon" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
            stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
            <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
        </svg>
        <svg id="sun-icon" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
            stroke-linecap="round" stroke-linejoin="round" style="display: none;">
            <circle cx="12" cy="12" r="5"></circle>
            <line x1="12" y1="1" x2="12" y2="3"></line>
            <line x1="12" y1="21" x2="12" y2="23"></line>
            <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
            <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
            <line x1="1" y1="12" x2="3" y2="12"></line>
            <line x1="21" y1="12" x2="23" y2="12"></line>
            <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
            <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
        </svg>
    </div>

    <nav class="top-navbar">
        <a href="/blogs/" class="nav-back" aria-label="Back to blogs">
            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none"
                stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"
                class="lucide lucide-arrow-left">
                <path d="m12 19-7-7 7-7" />
                <path d="M19 12H5" />
            </svg>
        </a>
        <div class="nav-links">
            <a href="/blogs/" class="nav-item active" data-section="blogs">blog</a>
            <a href="/projects/" class="nav-item" data-section="projects">work</a>
            <a href="/papers/" class="nav-item" data-section="papers">papers</a>
        </div>
        <button class="hamburger" id="hamburger-menu" aria-label="Menu">
            <span></span>
            <span></span>
            <span></span>
        </button>
    </nav>

    <div class="container">
        <main>
            <article class="blog-post-container">
                <header class="blog-post-header">
                    <h1 class="blog-post-title">How Do LLMs Decide the Next Token?</h1>
                    <div class="blog-post-meta">
                        <span>By Prashant Pandey</span>
                        <span>Sep 20, 2025</span>
                    </div>
                </header>

                <img src="cover.png" alt="How Do LLMs Decide the Next Token Cover"
                    style="width: 100%; margin-bottom: 1.5rem;">

                <div class="blog-post-content">
                    <p>Large Language Models (LLMs) like ChatGPT, Gemini, or Claude generate text one piece at a time.
                        They don't write full sentences in one go. Instead, they decide the <strong>next token</strong>,
                        add it to the
                        text, then repeat the process again and again until the response is complete.</p>

                    <p>But how do they decide what token comes next? The answer lies in a sequence of steps:
                        tokenization, numerical representation, neural network processing, probability prediction, and
                        finally sampling or selection. Let's go through each step in detail.</p>

                    <h3>Step 1: Breaking Text Into Tokens</h3>

                    <p>LLMs never work directly with plain words or sentences. They break down text into
                        <strong>tokens</strong>.
                    </p>

                    <p>A token is the smallest unit of text the model understands. Depending on the tokenizer, a token
                        might be:</p>

                    <ul>
                        <li>A full word (dog)</li>
                        <li>Part of a word (play → pla + y)</li>
                        <li>Punctuation (. or ,)</li>
                        <li>Special symbols (like &lt;PAD&gt; or &lt;EOS&gt; for padding and end-of-sequence)</li>
                    </ul>

                    <p>This process is called <strong>tokenization</strong>.</p>

                    <img src="1.png" alt="Tokenization Diagram">

                    <p>example:</p>

                    <pre><code class="language-python">Sentence: "LLMs are smart"
Tokens: ["L", "L", "Ms", " are", " smart"]</code></pre>

                    <p>This step is important because it allows LLMs to handle all kinds of inputs consistently,
                        including words they haven't seen before (by breaking them into smaller known parts).</p>

                    <h3>Step 2: Mapping Tokens to Numbers</h3>

                    <p>Once we have tokens, the model needs to work with them in numerical form. Each token in the
                        vocabulary is mapped to a unique <strong>ID number</strong>.</p>

                    <p>Example:</p>

                    <pre><code class="language-python">"L" → 1234
"L" → 5678
"Ms" → 9123
"are" → 45
"smart"→ 678</code></pre>

                    <p>This mapping forms the <strong>vocabulary</strong> of the model. Modern LLMs have vocabularies
                        with <strong>tens of
                            thousands of tokens</strong>.</p>

                    <img src="2.png" alt="Embedding Vectors Visualization">

                    <p>The IDs are then converted into <strong>embeddings, </strong>high dimensional vectors (arrays of
                        numbers) that
                        capture relationships between tokens. For example, embeddings for "king" and "queen" may be
                        close in vector space, because they are semantically related.</p>

                    <h3>Step 3: Processing With Transformer Neural Networks</h3>

                    <p>Now the embeddings go into the heart of the LLM, the <strong>transformer architecture</strong>.
                    </p>

                    <p>A transformer is made up of many layers. Each layer contains two key components:</p>

                    <ol>
                        <li><strong>Self-Attention</strong>: This mechanism allows the model to look at all previous
                            tokens and figure out
                            which ones are important when predicting the next token.</li>
                    </ol>

                    <ul>
                        <li>Example: In the sentence "The cat sat on the ___", the token "cat" is more important than
                            "the" for predicting "mat".</li>
                    </ul>

                    <p>2. <strong>Feedforward Neural Network</strong>: After attention, the information is passed
                        through fully connected
                        layers to refine the representation.</p>

                    <p>This process repeats across dozens or even hundreds of layers in large models. The deeper the
                        network, the richer the understanding of context.</p>

                    <h3>Step 4: Producing Probabilities for the Next Token</h3>

                    <p>At the final layer, the model outputs a <strong>probability distribution</strong> over the entire
                        vocabulary.</p>

                    <p>This is done using the <strong>softmax function</strong>, which turns raw scores (logits) into
                        probabilities that
                        add up to 1.</p>

                    <p>Example: If the input so far is "The capital of France is", the model might produce probabilities
                        like:</p>

                    <ul>
                        <li>"Paris" → 0.95</li>
                        <li>"London" → 0.02</li>
                        <li>"Berlin" → 0.01</li>
                        <li>"Madrid" → 0.01</li>
                        <li>Everything else → near 0</li>
                    </ul>
                    <p>This means the model thinks "Paris" is the most likely next token.</p>
                    <img src="3.gif" alt="Probability Distribution Visualization">

                </div>

                <div class="blog-post-tags">
                    <span class="blog-post-tag">large-language-models</span>
                    <span class="blog-post-tag">naturallanguageprocessing</span>
                    <span class="blog-post-tag">deep-learning</span>
                    <span class="blog-post-tag">ai-model-architecture</span>
                    <span class="blog-post-tag">text-generation</span>
                </div>
            </article>
        </main>
    </div>

    <script src="../../script.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
</body>

</html>